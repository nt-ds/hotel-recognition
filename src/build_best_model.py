# -*- coding: utf-8 -*-
"""build_best_model.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JxbMjCfWYRePOVTJliKj-ekVXnduCOPa
"""

import os
import warnings

from keras.applications import VGG16
from keras.callbacks import EarlyStopping, LearningRateScheduler, ModelCheckpoint, ReduceLROnPlateau, TerminateOnNaN
from keras.layers import Dense, Dropout, Flatten
from keras.layers.normalization import BatchNormalization
from keras.utils import to_categorical
from sklearn.model_selection import train_test_split

from keras import models

warnings.filterwarnings("ignore")

def build_best_model(training_images, training_labels, epochs=100, batch_size=20):
    
    '''
    Build the best model
    (VGG-16 Deep Convolutional Neural Network pre-trained on ImageNet database with
     the last 5 layers unfrozen to allow for additional training on hotel chain database)
    
    Arguments
    ---------
    training_images: numpy array of training images
    
    training_labels: numpy array of training labels
    
    Optional Arguments
    ------------------
    epochs:          number of epochs to train the model
                     if unspecified, epochs will default to 100
    
    batch_size:      number of samples per gradient update
                     if unspecified, batch_size will default to 20
    
    '''
    
    
    # VGG-16 pre-trained on ImageNet
    # not including the 3 fully-connected layers at the top of the network
    # applying global max pooling
    conv_base = VGG16(weights="imagenet", include_top=False, input_shape=(128, 128, 3), pooling=max)
    
    # VGG-16's last 5 layers unfrozen to allow for additional training on hotel chain database
    for layer in conv_base.layers[:-5]:
        layer.trainable = False

    # model architecture
    model = models.Sequential()
    model.add(conv_base)
    model.add(Flatten())
    model.add(Dense(512, activation="relu"))
    model.add(BatchNormalization())
    model.add(Dropout(0.5))
    model.add(Dense(256, activation="relu"))
    model.add(BatchNormalization())
    model.add(Dropout(0.5))
    model.add(Dense(128, activation="relu"))
    model.add(BatchNormalization())
    model.add(Dropout(0.5))
    model.add(Dense(3, activation="softmax"))

    # compile model
    model.compile(optimizer="Adam", loss="categorical_crossentropy", metrics=["acc"])
    model.summary()

    # callbacks
    ## save model and best weights
    saving_model_weights = ModelCheckpoint("best_model.hdf5", monitor="val_acc", verbose=0,
                                           save_best_only=True, save_weights_only=False, mode="auto", period=10)

    ## reduce learning rate when close to optimum
    reduce_lr = ReduceLROnPlateau(monitor="val_acc", factor=0.1, patience=20, verbose=0, mode="auto", min_delta=0.0001, min_lr=0)

    ## if NaN occurs, stop model
    nan_problem = TerminateOnNaN()

    ## stop training if validation accuracy is not changing or getting worse
    early_stop = EarlyStopping(monitor="val_acc", min_delta=0, patience=20, verbose=0, mode="auto", baseline=None, restore_best_weights=True)

    # fit model
    train_features, validation_features, train_labels, validation_labels = train_test_split(training_images, training_labels, test_size=0.2, random_state=42)
    train_labels_enc = to_categorical(train_labels, 3)
    validation_labels_enc = to_categorical(validation_labels, 3)
    model.fit(train_features, train_labels_enc, epochs=epochs, batch_size=batch_size, validation_data=(validation_features, validation_labels_enc),
              callbacks=[early_stop, nan_problem, reduce_lr, saving_model_weights])